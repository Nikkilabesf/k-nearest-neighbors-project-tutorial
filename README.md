<h1 align="center">ğŸ’–ğŸ· Wine Quality Classifier â€“ KNN Model ğŸ’»âœ¨</h1>

<p align="center">
  <i>A stylish machine learning project blending data, elegance, and science.</i><br>
  <b>Created by: Tenika Powell</b> ğŸŒ¸
</p>

---

## ğŸ’¡ Project Overview
This project uses **K-Nearest Neighbors (KNN)** to predict the quality of red wine based on its chemical makeup.  
Each wine sample includes properties like acidity, sugar, pH, and alcohol level â€” all used to determine if a wine is **Low**, **Medium**, or **High quality**.  

ğŸ‘©ğŸ½â€ğŸ”¬ Think of it as your AI-powered sommelier â€” picking the best bottle using pure data.

---

## ğŸ§  Machine Learning Pipeline
ğŸ’» **Steps used:**
1. âœ¨ Load dataset from the UCI Wine Quality Repository  
2. ğŸ‡ Convert the numeric `quality` into labeled categories (`0`, `1`, `2`)  
3. ğŸ§¼ Scale the data using `StandardScaler`  
4. ğŸ’– Train a **KNN model**  
5. ğŸ¯ Optimize with `GridSearchCV` (tested multiple `k`, `weights`, and `metrics`)  
6. ğŸ’« Evaluate with accuracy, confusion matrix, and classification report  
7. ğŸŒˆ Combine with an **Ensemble (VotingClassifier)** for an extra accuracy boost  

---

## ğŸ’» Tech Stack
| ğŸ§© Category | âš™ï¸ Tools |
|--------------|----------|
| Language | Python |
| Data Handling | Pandas, NumPy |
| ML Framework | Scikit-Learn |
| Visualization | Matplotlib, Seaborn |
| Optimization | GridSearchCV, VotingClassifier |

---

## ğŸ“Š Model Results
| ğŸ§  Model | âœ¨ Description | ğŸ¯ Accuracy |
|-----------|----------------|-------------|
| KNN (Default) | k=5 | ~0.56 |
| **KNN (Tuned)** | Best params via GridSearchCV | **~0.67** |
| **Voting Ensemble** | KNN + DecisionTree + LogisticRegression | **~0.70+** |

---

## ğŸŒ¸ Insights & Highlights
ğŸ’ Scaling the data significantly improved performance  
ğŸ’ â€œDistanceâ€ weighting helped nearby samples influence predictions  
ğŸ’ Ensemble models boosted accuracy and overall stability  
ğŸ’ The model performs beautifully on real-world-like classification problems

---

## ğŸ¨ Visualizations & Insights

### ğŸ‡ 1ï¸âƒ£ Feature Correlation Heatmap
<img src="images/correlation_heatmap.png" width="700"/>

Shows relationships among chemical features of wine.  
**Insight:** Alcohol, density, and sulphates have strong correlation with wine quality.

---

### ğŸ· 2ï¸âƒ£ Alcohol Content Distribution by Quality
<img src="images/alcohol_distribution.png" width="700"/>

Density plot comparing alcohol percentage in good vs. not-good wines.  
**Insight:** Good wines have noticeably higher alcohol levels.

---

### ğŸ’¡ 3ï¸âƒ£ Top 10 Most Important Features (XGBoost)
<img src="images/xgboost_importance.png" width="700"/>

Feature importance scores ranked by XGBoost model.  
**Insight:** Alcohol, sulphates, and density are top predictors of wine quality.

---

### ğŸ“Š 4ï¸âƒ£ Confusion Matrices for KNN and XGBoost
<img src="images/confusion_knn.png" width="350"/> <img src="images/confusion_xgb.png" width="350"/>

Compares how well each model predicted correct vs. incorrect classifications.  
**Insight:** XGBoost achieved better precision and recall on â€œGoodâ€ wines.

---

### ğŸ“ˆ 5ï¸âƒ£ ROCâ€“AUC Curve
<img src="images/roc_curve.png" width="700"/>

Illustrates trade-off between true positive and false positive rates.  
**Insight:** XGBoost achieved **AUC â‰ˆ 0.90**, showing strong separability between good and not-good wines.

---

### ğŸ” 6ï¸âƒ£ Precisionâ€“Recall Curve
<img src="images/pr_curve.png" width="700"/>

Demonstrates precision-recall relationship for both models.  
**Insight:** XGBoost maintains high precision even as recall increases â€” excellent for detecting â€œGoodâ€ wines.

---

### ğŸ’– 7ï¸âƒ£ Accuracy Comparison Bar Plot
<img src="images/accuracy_comparison.png" width="600"/>

Side-by-side accuracy comparison between KNN and XGBoost.  
**Result:**  
ğŸ¯ KNN â†’ ~79%  
âš¡ XGBoost â†’ ~85%

---

### ğŸ’ 8ï¸âƒ£ Model Performance Radar Chart
<img src="images/radar_chart.png" width="600"/>

Summarizes **Accuracy**, **ROCâ€“AUC**, and **PRâ€“AUC** across models.  
**Insight:** XGBoost outperforms across all three dimensions.

---

### ğŸŒˆ 9ï¸âƒ£ Overall Takeaway
> ğŸ“Š Using data balancing, feature engineering, and boosted algorithms,  
> the final model achieved **85% accuracy with 0.90+ AUC**, proving that chemistry can indeed predict taste!

---

ğŸ–¼ï¸ *Note:*  
If you want these plots to show directly in your GitHub README:  
- Save each image from your notebook with:
  ```python
  plt.savefig("images/<plot_name>.png", dpi=300, bbox_inches="tight")


---

## ğŸŒ· Next Steps
- Add **Random Forest or Gradient Boosting** comparisons ğŸŒ³  
- Build a **Streamlit / Gradio web app** for live predictions ğŸ’…  
- Expand to include **white wine** data ğŸ¾  

---

## ğŸ‘©ğŸ½â€ğŸ’» Author
**Tenika Powell**  
ğŸ’¼ *Machine Learning Engineer in Training*  
ğŸ“« [Powell.tenika.n@gmail.com](mailto:Powell.tenika.n@gmail.com)  
ğŸ’» [GitHub â€“ Nikkilabesf](https://github.com/Nikkilabesf)  
ğŸŒ¸ Passionate about AI, healthcare, and elegant code

---

## ğŸ’« Final Thoughts
This project showcases how **data science can be both analytical and aesthetic**.  
With the perfect mix of algorithms, creativity, and curiosity â€”  
we turned chemistry into **AI-powered taste testing** ğŸ·ğŸ’–

<p align="center">âœ¨ Data. Beauty. Precision. âœ¨</p>
